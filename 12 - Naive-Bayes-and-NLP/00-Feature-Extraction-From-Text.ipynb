{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e361937",
   "metadata": {},
   "source": [
    "## Feature extraction from text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4135332b",
   "metadata": {},
   "source": [
    "This notebook is divided into two sections:\n",
    "* First, we'll find out what what is necessary to build an NLP system that can turn a body of text into a numerical array of *features* by manually calcuating frequencies and building out TF-IDF.\n",
    "* Next we'll show how to perform these steps using scikit-learn tools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65d8e44",
   "metadata": {},
   "source": [
    "# Part One: Core Concepts on Feature Extraction\n",
    "\n",
    "\n",
    "In this section we'll use basic Python to build a rudimentary NLP system. We'll build a *corpus of documents* (two small text files), create a *vocabulary* from all the words in both documents, and then demonstrate a *Bag of Words* technique to extract features from each document.<br>\n",
    "<div class=\"alert alert-info\" style=\"margin: 20px\">This first section is for illustration only!\n",
    "<br>Don't worry about memorizing this code - later on we will let Scikit-Learn Preprocessing tools do this for us.</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4053ceae",
   "metadata": {},
   "source": [
    "## Start with some documents:\n",
    "For simplicity we won't use any punctuation in the text files One.txt and Two.txt. Let's quickly open them and read them. Keep in mind, you should avoid opening and reading entire files if they are very large, as Python could just display everything depending on how you open the file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4b6b027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a story about dogs\n",
      "our canine pets\n",
      "Dogs are furry animals\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('..\\data\\One.txt') as mytext:\n",
    "    a = mytext.read()\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b13b3c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is a story about dogs\\n', 'our canine pets\\n', 'Dogs are furry animals\\n']\n"
     ]
    }
   ],
   "source": [
    "# readlines returns as list\n",
    "with open('..\\data\\One.txt') as mytext:\n",
    "    a = mytext.readlines()\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff2cd3d",
   "metadata": {},
   "source": [
    "### Reading entire text as a string / Membaca keseluruan text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed8d3158",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('..\\data\\Two.txt') as mytext:\n",
    "    entire_text = mytext.read()\n",
    "    entire_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cbf850b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This story is about surfing\n",
      "Catching waves is fun\n",
      "Surfing is a popular water sport\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(entire_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef16ffb5",
   "metadata": {},
   "source": [
    "### Reading Each List as list /Membaca Setiap Baris sebagai Daftar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "493cbe73",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('..\\data\\One.txt') as mytext:\n",
    "    lines = mytext.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b5ebcab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is a story about dogs\\n',\n",
       " 'our canine pets\\n',\n",
       " 'Dogs are furry animals\\n']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327feb68",
   "metadata": {},
   "source": [
    "### Reading in Words Separately / Membaca secara terpisah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddcb525e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('..\\data\\One.txt') as mytext:\n",
    "    words = mytext.read().lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbf5e618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'story',\n",
       " 'about',\n",
       " 'dogs',\n",
       " 'our',\n",
       " 'canine',\n",
       " 'pets',\n",
       " 'dogs',\n",
       " 'are',\n",
       " 'furry',\n",
       " 'animals']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c150cb",
   "metadata": {},
   "source": [
    "## 2) Building a vocabulary (Creating a \"Bag of Words\")\n",
    "Let's create dictionaries that correspond to unique mappings of the words in the documents. We can begin to think of this as mapping out all the possible words available for all (both) documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d10e6d",
   "metadata": {},
   "source": [
    "### Read in one text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be517373",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('..\\data\\One.txt') as mytext:\n",
    "    words_one = mytext.read().lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "112e94ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'story',\n",
       " 'about',\n",
       " 'dogs',\n",
       " 'our',\n",
       " 'canine',\n",
       " 'pets',\n",
       " 'dogs',\n",
       " 'are',\n",
       " 'furry',\n",
       " 'animals']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47682c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482fe55d",
   "metadata": {},
   "source": [
    "### 2.1) Getting the unique words only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1970db68",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words_one = set(words_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39b7bbb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'animals',\n",
       " 'are',\n",
       " 'canine',\n",
       " 'dogs',\n",
       " 'furry',\n",
       " 'is',\n",
       " 'our',\n",
       " 'pets',\n",
       " 'story',\n",
       " 'this'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45302899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_words_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ff627c",
   "metadata": {},
   "source": [
    "Now we only have 12 unique words instead of original 13 words in Document one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cb0d97",
   "metadata": {},
   "source": [
    "### Rapeat for Two text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b6a5772",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('..\\data\\Two.txt') as mytext:\n",
    "    words_two = mytext.read().lower().split()\n",
    "    unique_words_two = set(words_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98c4b530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 12)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_two), len(unique_words_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd12742",
   "metadata": {},
   "source": [
    "### 2.2) Get all unique words across all documents (both One and Two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "658fd7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_unique_words = set()\n",
    "\n",
    "all_unique_words.update(unique_words_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90e8f12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dogs', 'story', 'are', 'a', 'furry', 'our', 'animals', 'this', 'about', 'pets', 'is', 'canine'}\n"
     ]
    }
   ],
   "source": [
    "print(all_unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a61a60ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_unique_words.update(unique_words_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "705f1a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dogs', 'a', 'furry', 'fun', 'our', 'surfing', 'this', 'about', 'is', 'canine', 'sport', 'story', 'are', 'popular', 'catching', 'water', 'animals', 'pets', 'waves'}\n"
     ]
    }
   ],
   "source": [
    "print(all_unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baed37e",
   "metadata": {},
   "source": [
    "### 2.3) Create vocab dictionary with related index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b25d4406",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_vocab = {}\n",
    "i = 0\n",
    "\n",
    "for words in all_unique_words:\n",
    "    full_vocab[words] = i\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ac0b2b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dogs': 0,\n",
       " 'a': 1,\n",
       " 'furry': 2,\n",
       " 'fun': 3,\n",
       " 'our': 4,\n",
       " 'surfing': 5,\n",
       " 'this': 6,\n",
       " 'about': 7,\n",
       " 'is': 8,\n",
       " 'canine': 9,\n",
       " 'sport': 10,\n",
       " 'story': 11,\n",
       " 'are': 12,\n",
       " 'popular': 13,\n",
       " 'catching': 14,\n",
       " 'water': 15,\n",
       " 'animals': 16,\n",
       " 'pets': 17,\n",
       " 'waves': 18}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c2aefd",
   "metadata": {},
   "source": [
    "### 3)Bag of Words to Frequency Counts\n",
    "Now that we've encapsulated our \"entire language\" in a dictionary, let's perform feature extraction on each of our original documents:\n",
    "\n",
    "### Empty counts per doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47af7eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_freq = [0] * len(full_vocab)\n",
    "two_freq = [0] * len(full_vocab)\n",
    "all_words = [''] * len(full_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad75da83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e1076ef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b2f553f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fe8f3a",
   "metadata": {},
   "source": [
    "### 3.1) Make A list of All Vocab (which will be used to map later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a8aa563",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in full_vocab:\n",
    "    word_index = full_vocab[word]\n",
    "    all_words[word_index] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "406cefaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dogs', 'a', 'furry', 'fun', 'our', 'surfing', 'this', 'about', 'is', 'canine', 'sport', 'story', 'are', 'popular', 'catching', 'water', 'animals', 'pets', 'waves']\n"
     ]
    }
   ],
   "source": [
    "print(all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a12352",
   "metadata": {},
   "source": [
    "### 3.2) Add in counts per word per doc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9665030d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Data/One.txt') as file:\n",
    "    one_text  = file.read().lower().split()\n",
    "    \n",
    "for word in one_text:\n",
    "    word_index = full_vocab[word] #get the index of that specific word\n",
    "    one_freq[word_index]+= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "67c1cdc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d970c51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('..\\data\\Two.txt') as file:\n",
    "    two_text = file.read().lower().split()\n",
    "    \n",
    "for word in two_text:\n",
    "    word_index = full_vocab[word]\n",
    "    two_freq[word_index]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f15d4d71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 1, 0, 2, 1, 1, 3, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427bd99c",
   "metadata": {},
   "source": [
    "### 3.3) Create the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "23d23f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "497683a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dogs</th>\n",
       "      <th>a</th>\n",
       "      <th>furry</th>\n",
       "      <th>fun</th>\n",
       "      <th>our</th>\n",
       "      <th>surfing</th>\n",
       "      <th>this</th>\n",
       "      <th>about</th>\n",
       "      <th>is</th>\n",
       "      <th>canine</th>\n",
       "      <th>sport</th>\n",
       "      <th>story</th>\n",
       "      <th>are</th>\n",
       "      <th>popular</th>\n",
       "      <th>catching</th>\n",
       "      <th>water</th>\n",
       "      <th>animals</th>\n",
       "      <th>pets</th>\n",
       "      <th>waves</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dogs  a  furry  fun  our  surfing  this  about  is  canine  sport  story  \\\n",
       "0     2  1      1    0    1        0     1      1   1       1      0      1   \n",
       "1     0  1      0    1    0        2     1      1   3       0      1      1   \n",
       "\n",
       "   are  popular  catching  water  animals  pets  waves  \n",
       "0    1        0         0      0        1     1      0  \n",
       "1    0        1         1      1        0     0      1  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data =[one_freq, two_freq], columns = all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65618ab",
   "metadata": {},
   "source": [
    "Now we can how frequently each word appears in the documents.\n",
    "\n",
    "By comparing the vectors we see that some words are common to both, some appear only in One.txt, others only in Two.txt. Extending this logic to tens of thousands of documents, we would see the vocabulary dictionary grow to hundreds of thousands of words. Vectors would contain mostly zero values, making them `sparse matrices.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d075a2",
   "metadata": {},
   "source": [
    "## Concepts to Consider:\n",
    "## Bag of Words and Tf-idf\n",
    "\n",
    "Dalam contoh di atas, setiap vektor dapat dianggap sebagai sekumpulan kata. Hal ini mungkin tidak berguna sampai kita mempertimbangkan frekuensi istilah, atau seberapa sering setiap kata muncul dalam dokumen. Cara sederhana untuk menghitung frekuensi istilah adalah dengan membagi jumlah kemunculan suatu kata dengan jumlah total kata dalam dokumen. Dengan cara ini, frekuensi kemunculan sebuah kata dalam dokumen besar dapat dibandingkan dengan frekuensi kemunculan dokumen kecil.\n",
    "\n",
    "Namun, mungkin sulit untuk membedakan dokumen berdasarkan frekuensi istilah jika sebuah kata muncul di sebagian besar dokumen. Untuk menangani hal ini kami juga mempertimbangkan invers frekuensi dokumen, yaitu jumlah total dokumen dibagi dengan jumlah dokumen yang mengandung kata tersebut. Dalam praktiknya, kami mengonversi nilai ini ke skala logaritmik, seperti yang dijelaskan di sini.\n",
    "\n",
    "Bersama-sama istilah-istilah ini menjadi tf-idf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e38485b",
   "metadata": {},
   "source": [
    "## Stop Words and Word Stems\n",
    "Beberapa kata seperti \"the\" dan \"and\" muncul begitu sering, dan dalam banyak dokumen, sehingga kita tidak perlu repot-repot menghitungnya. Selain itu, mungkin masuk akal jika hanya mencatat akar kata, misalnya `cat`, sebagai ganti `cat` dan `cats`. Ini akan memperkecil susunan kosakata kita dan meningkatkan kinerja."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bfec7f",
   "metadata": {},
   "source": [
    "## Tokenization and Tagging\n",
    "\n",
    "Saat kami membuat vektor, hal pertama yang kami lakukan adalah membagi teks masuk menjadi spasi dengan `.split()`. Ini adalah bentuk tokenisasi yang kasar - yaitu, membagi dokumen menjadi kata-kata individual. Dalam contoh sederhana ini kami tidak mengkhawatirkan tanda baca atau jenis kata yang berbeda. Di dunia nyata, kami mengandalkan morfologi yang cukup canggih untuk mengurai teks dengan tepat.\n",
    "\n",
    "Setelah teks dibagi, kita dapat kembali dan menandai token kita dengan informasi tentang jenis kata, ketergantungan tata bahasa, dll. Hal ini menambah lebih banyak dimensi pada data kita dan memungkinkan pemahaman yang lebih mendalam tentang konteks dokumen tertentu. Oleh karena itu, vektor menjadi **high dimensional sparse matrices.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4edc722",
   "metadata": {},
   "source": [
    "--------\n",
    "---\n",
    "## Part Two: Feature Extraction with Scikit-Learn\n",
    "Let's explore the more realistic process of using sklearn to complete the tasks mentioned above!\n",
    "\n",
    "## Scikit-Learn's Text Feature Extraction Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "213d5e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ['This is a line',\n",
    "           \"This is another line\",\n",
    "       \"Completely different line\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0580fb",
   "metadata": {},
   "source": [
    "## Feature eXtractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ee8e93a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c882fc9",
   "metadata": {},
   "source": [
    "## CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818e4815",
   "metadata": {},
   "source": [
    "* cv akan memperlakukan setiap nilai sebagai satu dokumen\n",
    "* `fit_transform` pada dasarnya mendapatkan kosakata unik yang sesuai dan kemudian mengubahnya dengan melakukan penghitungan frekuensi secara aktual pada setiap dokumen di dalam daftar itu.\n",
    "* dan itu mengembalikan `sparse matrix`. Alasannya adalah ketika melakukan vektorisasi dan pembuatan model kantong kata, yang akan terjadi adalah sebagian besar item dalam matriks akan bernilai nol. Jadi ketika Anda berurusan dengan ratusan dan ribuan dokumen dengan banyak kata, Anda ingin memastikan bahwa Anda tidak memakan terlalu banyak memori PC jika tidak perlu hanya dengan menyimpan sekumpulan angka nol. Itulah sebabnya kami memiliki matriks renggang.\n",
    "* `sparse matriks`  dengan matriks 3x6. Mengapa 3? karena ada 3 dokumen dalam daftar yang kami lewati. Ketika kita menggunakan metode `todense()`, kita dapat melihat jumlah frekuensi yang disimpan asli yang tidak dalam bentuk matriks renggang (yang menyimpan informasi dengan cara yang efisien dalam memori). **CATATAN: kami tidak ingin memanggil metode ini jika kami memiliki nilai kata yang besar, yang akan memakan banyak ruang memori**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9eaf20ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "137ba054",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_matrix = cv.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f06740ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x6 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 10 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8328899d",
   "metadata": {},
   "source": [
    "### Use todense() to see original form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6ad9bdc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, 1, 1, 1],\n",
       "        [1, 0, 0, 1, 1, 1],\n",
       "        [0, 1, 1, 0, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_matrix.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2f54f6",
   "metadata": {},
   "source": [
    "### Vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0d09c690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 5, 'is': 3, 'line': 4, 'another': 0, 'completely': 1, 'different': 2}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5557052b",
   "metadata": {},
   "source": [
    "Jika kita perhatikan lebih dekat nilainya, yang lain ada di indeks 0. Jadi jika melihat hasil todense(), lihat indeks 0 memiliki nilai 1 pada dokumen kedua. yang masuk akal karena Ini adalah baris lain adalah dokumen kedua yang memiliki kata lain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1f8b6c",
   "metadata": {},
   "source": [
    "## stop_words parameter\n",
    "with the use of this parameter, common stop words in English are not longer part of the vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2d65ad40",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8100bbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_matrix = cv.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1c4620ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'line': 2, 'completely': 0, 'different': 1}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8545b4bb",
   "metadata": {},
   "source": [
    "-----\n",
    "## TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c2d98eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transform = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2636e85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5df96f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = cv.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "02d712e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x6 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 10 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "691a50fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = tfidf_transform.fit_transform(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c5922348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.        , 0.        , 0.61980538, 0.48133417,\n",
       "         0.61980538],\n",
       "        [0.63174505, 0.        , 0.        , 0.4804584 , 0.37311881,\n",
       "         0.4804584 ],\n",
       "        [0.        , 0.65249088, 0.65249088, 0.        , 0.38537163,\n",
       "         0.        ]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac29c45",
   "metadata": {},
   "source": [
    "## Use Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8567fc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1cde19c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('cv', CountVectorizer()), ('tfidf',TfidfTransformer())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a84b5846",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pipe.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e20fafa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x6 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 10 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "10fedfb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.        , 0.        , 0.61980538, 0.48133417,\n",
       "         0.61980538],\n",
       "        [0.63174505, 0.        , 0.        , 0.4804584 , 0.37311881,\n",
       "         0.4804584 ],\n",
       "        [0.        , 0.65249088, 0.65249088, 0.        , 0.38537163,\n",
       "         0.        ]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb737f0",
   "metadata": {},
   "source": [
    "## TfidfVectorier\n",
    "Lakukan keduanya di atas dalam satu langkah!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6089f674",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "280c9b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "new = tfidf.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d6f96784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.        , 0.        , 0.61980538, 0.48133417,\n",
       "         0.61980538],\n",
       "        [0.63174505, 0.        , 0.        , 0.4804584 , 0.37311881,\n",
       "         0.4804584 ],\n",
       "        [0.        , 0.65249088, 0.65249088, 0.        , 0.38537163,\n",
       "         0.        ]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011e8ec2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
