{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07f04684",
   "metadata": {},
   "source": [
    "## Regularization with SciKit-Learn\n",
    "Previously we created a new polynomial feature set and then applied our standard linear regression on it, but we can be smarter about model choice and utilize regularization.\n",
    "\n",
    "Regularization attempts to minimize the RSS (residual sum of squares) and a penalty factor. This penalty factor will penalize models that have coefficients that are too large. Some methods of regularization will actually cause non useful features to have a coefficient of zero, in which case the model does not consider the feature.\n",
    "\n",
    "Let's explore two methods of regularization, Ridge Regression and Lasso. We'll combine these with the polynomial feature set (it wouldn't be as effective to perform regularization of a model on such a small original feature set of the original X)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a52c719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scienceplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89728156",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('..\\data\\Advertising.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6da5c3f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TV</th>\n",
       "      <th>radio</th>\n",
       "      <th>newspaper</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>230.1</td>\n",
       "      <td>37.8</td>\n",
       "      <td>69.2</td>\n",
       "      <td>22.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44.5</td>\n",
       "      <td>39.3</td>\n",
       "      <td>45.1</td>\n",
       "      <td>10.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17.2</td>\n",
       "      <td>45.9</td>\n",
       "      <td>69.3</td>\n",
       "      <td>9.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>151.5</td>\n",
       "      <td>41.3</td>\n",
       "      <td>58.5</td>\n",
       "      <td>18.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>180.8</td>\n",
       "      <td>10.8</td>\n",
       "      <td>58.4</td>\n",
       "      <td>12.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TV  radio  newspaper  sales\n",
       "0  230.1   37.8       69.2   22.1\n",
       "1   44.5   39.3       45.1   10.4\n",
       "2   17.2   45.9       69.3    9.3\n",
       "3  151.5   41.3       58.5   18.5\n",
       "4  180.8   10.8       58.4   12.9"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f36231",
   "metadata": {},
   "source": [
    "## Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e251609c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('sales',axis=1)\n",
    "y = df['sales']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9d0a88",
   "metadata": {},
   "source": [
    "## Polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5bb355f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95a81ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_converter = PolynomialFeatures(degree= 3, include_bias = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a2b632d",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_features = poly_converter.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bfd0d9",
   "metadata": {},
   "source": [
    "## Train | Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31a533ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "291ed0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(poly_features, y, test_size= 0.3, random_state = 101)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf8be06",
   "metadata": {},
   "source": [
    "## Scaling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a12b7976",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63901b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47081f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1397b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e3f2162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.49300171, -0.33994238,  1.61586707, ..., -0.16360242,\n",
       "         0.54694754,  1.37075536],\n",
       "       [ 1.61337153,  1.32852213,  1.90079242, ...,  2.63236858,\n",
       "         2.6297449 ,  1.95593378],\n",
       "       [-0.84811893, -1.58789957, -1.02319564, ..., -0.72634944,\n",
       "        -0.61593941, -0.54133745],\n",
       "       ...,\n",
       "       [ 1.04606563, -1.30982215,  0.27064572, ..., -0.71065643,\n",
       "        -0.53420112, -0.20690952],\n",
       "       [ 0.74817069,  0.03987068, -1.26608283, ..., -0.66805936,\n",
       "        -0.61031703, -0.54616941],\n",
       "       [ 0.13813882,  0.55533126,  1.01799092, ...,  0.61006767,\n",
       "         0.67881488,  0.45425942]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b627703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.91339472,  1.76259419,  0.68168558, ...,  2.09516506,\n",
       "         0.89465642,  0.09868885],\n",
       "       [ 1.23121149,  0.23655959, -1.19601922, ..., -0.62566104,\n",
       "        -0.60402516, -0.54547726],\n",
       "       [-0.46002473, -0.40776614,  0.35005115, ..., -0.43068575,\n",
       "        -0.23635113, -0.15776394],\n",
       "       ...,\n",
       "       [-1.33115973,  1.19965698, -1.18200649, ..., -0.47847952,\n",
       "        -0.59569585, -0.54528297],\n",
       "       [ 1.3522684 , -0.23142436, -1.4389064 , ..., -0.7233135 ,\n",
       "        -0.61591305, -0.54660193],\n",
       "       [ 0.27818504, -0.06186496,  0.01841672, ..., -0.32290124,\n",
       "        -0.28905624, -0.33528306]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cad7197",
   "metadata": {},
   "source": [
    "## Rigde Regulation (L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "492144bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db01518b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Ridge in module sklearn.linear_model._ridge:\n",
      "\n",
      "class Ridge(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, _BaseRidge)\n",
      " |  Ridge(alpha=1.0, *, fit_intercept=True, normalize='deprecated', copy_X=True, max_iter=None, tol=0.001, solver='auto', positive=False, random_state=None)\n",
      " |  \n",
      " |  Linear least squares with l2 regularization.\n",
      " |  \n",
      " |  Minimizes the objective function::\n",
      " |  \n",
      " |  ||y - Xw||^2_2 + alpha * ||w||^2_2\n",
      " |  \n",
      " |  This model solves a regression model where the loss function is\n",
      " |  the linear least squares function and regularization is given by\n",
      " |  the l2-norm. Also known as Ridge Regression or Tikhonov regularization.\n",
      " |  This estimator has built-in support for multi-variate regression\n",
      " |  (i.e., when y is a 2d-array of shape (n_samples, n_targets)).\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <ridge_regression>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  alpha : {float, ndarray of shape (n_targets,)}, default=1.0\n",
      " |      Regularization strength; must be a positive float. Regularization\n",
      " |      improves the conditioning of the problem and reduces the variance of\n",
      " |      the estimates. Larger values specify stronger regularization.\n",
      " |      Alpha corresponds to ``1 / (2C)`` in other linear models such as\n",
      " |      :class:`~sklearn.linear_model.LogisticRegression` or\n",
      " |      :class:`~sklearn.svm.LinearSVC`. If an array is passed, penalties are\n",
      " |      assumed to be specific to the targets. Hence they must correspond in\n",
      " |      number.\n",
      " |  \n",
      " |  fit_intercept : bool, default=True\n",
      " |      Whether to fit the intercept for this model. If set\n",
      " |      to false, no intercept will be used in calculations\n",
      " |      (i.e. ``X`` and ``y`` are expected to be centered).\n",
      " |  \n",
      " |  normalize : bool, default=False\n",
      " |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      " |      If True, the regressors X will be normalized before regression by\n",
      " |      subtracting the mean and dividing by the l2-norm.\n",
      " |      If you wish to standardize, please use\n",
      " |      :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      " |      on an estimator with ``normalize=False``.\n",
      " |  \n",
      " |      .. deprecated:: 1.0\n",
      " |          ``normalize`` was deprecated in version 1.0 and\n",
      " |          will be removed in 1.2.\n",
      " |  \n",
      " |  copy_X : bool, default=True\n",
      " |      If True, X will be copied; else, it may be overwritten.\n",
      " |  \n",
      " |  max_iter : int, default=None\n",
      " |      Maximum number of iterations for conjugate gradient solver.\n",
      " |      For 'sparse_cg' and 'lsqr' solvers, the default value is determined\n",
      " |      by scipy.sparse.linalg. For 'sag' solver, the default value is 1000.\n",
      " |      For 'lbfgs' solver, the default value is 15000.\n",
      " |  \n",
      " |  tol : float, default=1e-3\n",
      " |      Precision of the solution.\n",
      " |  \n",
      " |  solver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg',             'sag', 'saga', 'lbfgs'}, default='auto'\n",
      " |      Solver to use in the computational routines:\n",
      " |  \n",
      " |      - 'auto' chooses the solver automatically based on the type of data.\n",
      " |  \n",
      " |      - 'svd' uses a Singular Value Decomposition of X to compute the Ridge\n",
      " |        coefficients. More stable for singular matrices than 'cholesky'.\n",
      " |  \n",
      " |      - 'cholesky' uses the standard scipy.linalg.solve function to\n",
      " |        obtain a closed-form solution.\n",
      " |  \n",
      " |      - 'sparse_cg' uses the conjugate gradient solver as found in\n",
      " |        scipy.sparse.linalg.cg. As an iterative algorithm, this solver is\n",
      " |        more appropriate than 'cholesky' for large-scale data\n",
      " |        (possibility to set `tol` and `max_iter`).\n",
      " |  \n",
      " |      - 'lsqr' uses the dedicated regularized least-squares routine\n",
      " |        scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative\n",
      " |        procedure.\n",
      " |  \n",
      " |      - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses\n",
      " |        its improved, unbiased version named SAGA. Both methods also use an\n",
      " |        iterative procedure, and are often faster than other solvers when\n",
      " |        both n_samples and n_features are large. Note that 'sag' and\n",
      " |        'saga' fast convergence is only guaranteed on features with\n",
      " |        approximately the same scale. You can preprocess the data with a\n",
      " |        scaler from sklearn.preprocessing.\n",
      " |  \n",
      " |      - 'lbfgs' uses L-BFGS-B algorithm implemented in\n",
      " |        `scipy.optimize.minimize`. It can be used only when `positive`\n",
      " |        is True.\n",
      " |  \n",
      " |      All last six solvers support both dense and sparse data. However, only\n",
      " |      'sag', 'sparse_cg', and 'lbfgs' support sparse input when `fit_intercept`\n",
      " |      is True.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         Stochastic Average Gradient descent solver.\n",
      " |      .. versionadded:: 0.19\n",
      " |         SAGA solver.\n",
      " |  \n",
      " |  positive : bool, default=False\n",
      " |      When set to ``True``, forces the coefficients to be positive.\n",
      " |      Only 'lbfgs' solver is supported in this case.\n",
      " |  \n",
      " |  random_state : int, RandomState instance, default=None\n",
      " |      Used when ``solver`` == 'sag' or 'saga' to shuffle the data.\n",
      " |      See :term:`Glossary <random_state>` for details.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         `random_state` to support Stochastic Average Gradient.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  coef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n",
      " |      Weight vector(s).\n",
      " |  \n",
      " |  intercept_ : float or ndarray of shape (n_targets,)\n",
      " |      Independent term in decision function. Set to 0.0 if\n",
      " |      ``fit_intercept = False``.\n",
      " |  \n",
      " |  n_iter_ : None or ndarray of shape (n_targets,)\n",
      " |      Actual number of iterations for each target. Available only for\n",
      " |      sag and lsqr solvers. Other solvers will return None.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |  \n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      " |      has feature names that are all strings.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  RidgeClassifier : Ridge classifier.\n",
      " |  RidgeCV : Ridge regression with built-in cross validation.\n",
      " |  :class:`~sklearn.kernel_ridge.KernelRidge` : Kernel ridge regression\n",
      " |      combines ridge regression with the kernel trick.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.linear_model import Ridge\n",
      " |  >>> import numpy as np\n",
      " |  >>> n_samples, n_features = 10, 5\n",
      " |  >>> rng = np.random.RandomState(0)\n",
      " |  >>> y = rng.randn(n_samples)\n",
      " |  >>> X = rng.randn(n_samples, n_features)\n",
      " |  >>> clf = Ridge(alpha=1.0)\n",
      " |  >>> clf.fit(X, y)\n",
      " |  Ridge()\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Ridge\n",
      " |      sklearn.base.MultiOutputMixin\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      _BaseRidge\n",
      " |      sklearn.linear_model._base.LinearModel\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, alpha=1.0, *, fit_intercept=True, normalize='deprecated', copy_X=True, max_iter=None, tol=0.001, solver='auto', positive=False, random_state=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit Ridge regression model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Training data.\n",
      " |      \n",
      " |      y : ndarray of shape (n_samples,) or (n_samples, n_targets)\n",
      " |          Target values.\n",
      " |      \n",
      " |      sample_weight : float or ndarray of shape (n_samples,), default=None\n",
      " |          Individual weights for each sample. If given a float, every sample\n",
      " |          will have the same weight.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Fitted estimator.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the coefficient of determination of the prediction.\n",
      " |      \n",
      " |      The coefficient of determination :math:`R^2` is defined as\n",
      " |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      " |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      " |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      " |      The best possible score is 1.0 and it can be negative (because the\n",
      " |      model can be arbitrarily worse). A constant model that always predicts\n",
      " |      the expected value of `y`, disregarding the input features, would get\n",
      " |      a :math:`R^2` score of 0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples. For some estimators this may be a precomputed\n",
      " |          kernel matrix or a list of generic objects instead with shape\n",
      " |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      " |          is the number of samples used in the fitting for the estimator.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True values for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      " |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      " |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      " |      This influences the ``score`` method of all the multioutput\n",
      " |      regressors (except for\n",
      " |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict using the linear model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      " |          Samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array, shape (n_samples,)\n",
      " |          Returns predicted values.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "549ffb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_model = Ridge(alpha =10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52f78eca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=10)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ecff5140",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = ridge_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6591b92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluatian metrics\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a68065d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE = mean_absolute_error(y_test, test_predictions)\n",
    "MSE = mean_squared_error(y_test, test_predictions)\n",
    "RMSE = np.sqrt(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c331fe23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5774404204714173"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e231fc98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8003783071528403"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c59c10e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8946386461319678"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87149b9a",
   "metadata": {},
   "source": [
    "How did it perform on the training set? (This will be used later on for comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1d71d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5288348183025328"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training set performance\n",
    "train_prediction = ridge_model.predict(X_train)\n",
    "MAE = mean_absolute_error(y_train, train_prediction)\n",
    "MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deb9b37",
   "metadata": {},
   "source": [
    "## Choosing an aplha value with Cross_Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "93003fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "385a1710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class RidgeCV in module sklearn.linear_model._ridge:\n",
      "\n",
      "class RidgeCV(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, _BaseRidgeCV)\n",
      " |  RidgeCV(alphas=(0.1, 1.0, 10.0), *, fit_intercept=True, normalize='deprecated', scoring=None, cv=None, gcv_mode=None, store_cv_values=False, alpha_per_target=False)\n",
      " |  \n",
      " |  Ridge regression with built-in cross-validation.\n",
      " |  \n",
      " |  See glossary entry for :term:`cross-validation estimator`.\n",
      " |  \n",
      " |  By default, it performs efficient Leave-One-Out Cross-Validation.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <ridge_regression>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  alphas : ndarray of shape (n_alphas,), default=(0.1, 1.0, 10.0)\n",
      " |      Array of alpha values to try.\n",
      " |      Regularization strength; must be a positive float. Regularization\n",
      " |      improves the conditioning of the problem and reduces the variance of\n",
      " |      the estimates. Larger values specify stronger regularization.\n",
      " |      Alpha corresponds to ``1 / (2C)`` in other linear models such as\n",
      " |      :class:`~sklearn.linear_model.LogisticRegression` or\n",
      " |      :class:`~sklearn.svm.LinearSVC`.\n",
      " |      If using Leave-One-Out cross-validation, alphas must be positive.\n",
      " |  \n",
      " |  fit_intercept : bool, default=True\n",
      " |      Whether to calculate the intercept for this model. If set\n",
      " |      to false, no intercept will be used in calculations\n",
      " |      (i.e. data is expected to be centered).\n",
      " |  \n",
      " |  normalize : bool, default=False\n",
      " |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      " |      If True, the regressors X will be normalized before regression by\n",
      " |      subtracting the mean and dividing by the l2-norm.\n",
      " |      If you wish to standardize, please use\n",
      " |      :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      " |      on an estimator with ``normalize=False``.\n",
      " |  \n",
      " |      .. deprecated:: 1.0\n",
      " |          ``normalize`` was deprecated in version 1.0 and will be removed in\n",
      " |          1.2.\n",
      " |  \n",
      " |  scoring : str, callable, default=None\n",
      " |      A string (see model evaluation documentation) or\n",
      " |      a scorer callable object / function with signature\n",
      " |      ``scorer(estimator, X, y)``.\n",
      " |      If None, the negative mean squared error if cv is 'auto' or None\n",
      " |      (i.e. when using leave-one-out cross-validation), and r2 score\n",
      " |      otherwise.\n",
      " |  \n",
      " |  cv : int, cross-validation generator or an iterable, default=None\n",
      " |      Determines the cross-validation splitting strategy.\n",
      " |      Possible inputs for cv are:\n",
      " |  \n",
      " |      - None, to use the efficient Leave-One-Out cross-validation\n",
      " |      - integer, to specify the number of folds.\n",
      " |      - :term:`CV splitter`,\n",
      " |      - An iterable yielding (train, test) splits as arrays of indices.\n",
      " |  \n",
      " |      For integer/None inputs, if ``y`` is binary or multiclass,\n",
      " |      :class:`~sklearn.model_selection.StratifiedKFold` is used, else,\n",
      " |      :class:`~sklearn.model_selection.KFold` is used.\n",
      " |  \n",
      " |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      " |      cross-validation strategies that can be used here.\n",
      " |  \n",
      " |  gcv_mode : {'auto', 'svd', 'eigen'}, default='auto'\n",
      " |      Flag indicating which strategy to use when performing\n",
      " |      Leave-One-Out Cross-Validation. Options are::\n",
      " |  \n",
      " |          'auto' : use 'svd' if n_samples > n_features, otherwise use 'eigen'\n",
      " |          'svd' : force use of singular value decomposition of X when X is\n",
      " |              dense, eigenvalue decomposition of X^T.X when X is sparse.\n",
      " |          'eigen' : force computation via eigendecomposition of X.X^T\n",
      " |  \n",
      " |      The 'auto' mode is the default and is intended to pick the cheaper\n",
      " |      option of the two depending on the shape of the training data.\n",
      " |  \n",
      " |  store_cv_values : bool, default=False\n",
      " |      Flag indicating if the cross-validation values corresponding to\n",
      " |      each alpha should be stored in the ``cv_values_`` attribute (see\n",
      " |      below). This flag is only compatible with ``cv=None`` (i.e. using\n",
      " |      Leave-One-Out Cross-Validation).\n",
      " |  \n",
      " |  alpha_per_target : bool, default=False\n",
      " |      Flag indicating whether to optimize the alpha value (picked from the\n",
      " |      `alphas` parameter list) for each target separately (for multi-output\n",
      " |      settings: multiple prediction targets). When set to `True`, after\n",
      " |      fitting, the `alpha_` attribute will contain a value for each target.\n",
      " |      When set to `False`, a single alpha is used for all targets.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  cv_values_ : ndarray of shape (n_samples, n_alphas) or             shape (n_samples, n_targets, n_alphas), optional\n",
      " |      Cross-validation values for each alpha (only available if\n",
      " |      ``store_cv_values=True`` and ``cv=None``). After ``fit()`` has been\n",
      " |      called, this attribute will contain the mean squared errors if\n",
      " |      `scoring is None` otherwise it will contain standardized per point\n",
      " |      prediction values.\n",
      " |  \n",
      " |  coef_ : ndarray of shape (n_features) or (n_targets, n_features)\n",
      " |      Weight vector(s).\n",
      " |  \n",
      " |  intercept_ : float or ndarray of shape (n_targets,)\n",
      " |      Independent term in decision function. Set to 0.0 if\n",
      " |      ``fit_intercept = False``.\n",
      " |  \n",
      " |  alpha_ : float or ndarray of shape (n_targets,)\n",
      " |      Estimated regularization parameter, or, if ``alpha_per_target=True``,\n",
      " |      the estimated regularization parameter for each target.\n",
      " |  \n",
      " |  best_score_ : float or ndarray of shape (n_targets,)\n",
      " |      Score of base estimator with best alpha, or, if\n",
      " |      ``alpha_per_target=True``, a score for each target.\n",
      " |  \n",
      " |      .. versionadded:: 0.23\n",
      " |  \n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      " |      has feature names that are all strings.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  Ridge : Ridge regression.\n",
      " |  RidgeClassifier : Classifier based on ridge regression on {-1, 1} labels.\n",
      " |  RidgeClassifierCV : Ridge classifier with built-in cross validation.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.datasets import load_diabetes\n",
      " |  >>> from sklearn.linear_model import RidgeCV\n",
      " |  >>> X, y = load_diabetes(return_X_y=True)\n",
      " |  >>> clf = RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)\n",
      " |  >>> clf.score(X, y)\n",
      " |  0.5166...\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      RidgeCV\n",
      " |      sklearn.base.MultiOutputMixin\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      _BaseRidgeCV\n",
      " |      sklearn.linear_model._base.LinearModel\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the coefficient of determination of the prediction.\n",
      " |      \n",
      " |      The coefficient of determination :math:`R^2` is defined as\n",
      " |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      " |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      " |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      " |      The best possible score is 1.0 and it can be negative (because the\n",
      " |      model can be arbitrarily worse). A constant model that always predicts\n",
      " |      the expected value of `y`, disregarding the input features, would get\n",
      " |      a :math:`R^2` score of 0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples. For some estimators this may be a precomputed\n",
      " |          kernel matrix or a list of generic objects instead with shape\n",
      " |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      " |          is the number of samples used in the fitting for the estimator.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True values for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      " |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      " |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      " |      This influences the ``score`` method of all the multioutput\n",
      " |      regressors (except for\n",
      " |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _BaseRidgeCV:\n",
      " |  \n",
      " |  __init__(self, alphas=(0.1, 1.0, 10.0), *, fit_intercept=True, normalize='deprecated', scoring=None, cv=None, gcv_mode=None, store_cv_values=False, alpha_per_target=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit Ridge regression model with cv.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : ndarray of shape (n_samples, n_features)\n",
      " |          Training data. If using GCV, will be cast to float64\n",
      " |          if necessary.\n",
      " |      \n",
      " |      y : ndarray of shape (n_samples,) or (n_samples, n_targets)\n",
      " |          Target values. Will be cast to X's dtype if necessary.\n",
      " |      \n",
      " |      sample_weight : float or ndarray of shape (n_samples,), default=None\n",
      " |          Individual weights for each sample. If given a float, every sample\n",
      " |          will have the same weight.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Fitted estimator.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      When sample_weight is provided, the selected hyperparameter may depend\n",
      " |      on whether we use leave-one-out cross-validation (cv=None or cv='auto')\n",
      " |      or another form of cross-validation, because only leave-one-out\n",
      " |      cross-validation takes the sample weights into account when computing\n",
      " |      the validation score.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict using the linear model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      " |          Samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array, shape (n_samples,)\n",
      " |          Returns predicted values.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(RidgeCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "afd1cb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative RMSE so all metrics follow convention \"Higher is better\"\n",
    "\n",
    "# See all options: sklearn.metrics.SCORERS.keys()\n",
    "\n",
    "ridgecv_model = RidgeCV(alphas = (0.1, 1.0, 10.0), scoring ='neg_mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "70bb8223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RidgeCV(alphas=array([ 0.1,  1. , 10. ]), scoring='neg_mean_absolute_error')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridgecv_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "73a3f262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridgecv_model.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "89958e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = ridgecv_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "52042822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation metrics\n",
    "MAE = mean_absolute_error(y_test, test_predictions)\n",
    "MSE = mean_squared_error(y_test, test_predictions)\n",
    "RMSE = np.sqrt(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "de9c1bc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42737748843249934"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "91f0853a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6180719926923581"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d2e12c51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3094132105641705"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training set performance\n",
    "train_predictions = ridgecv_model.predict(X_train)\n",
    "MAE = mean_absolute_error(y_train, train_predictions)\n",
    "MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2d061b14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.40769392,  0.5885865 ,  0.40390395, -6.18263924,  4.59607939,\n",
       "       -1.18789654, -1.15200458,  0.57837796, -0.1261586 ,  2.5569777 ,\n",
       "       -1.38900471,  0.86059434,  0.72219553, -0.26129256,  0.17870787,\n",
       "        0.44353612, -0.21362436, -0.04622473, -0.06441449])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridgecv_model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd7c2c3",
   "metadata": {},
   "source": [
    "## Lasso Regression (L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7692ebad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ee99ab3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LassoCV in module sklearn.linear_model._coordinate_descent:\n",
      "\n",
      "class LassoCV(sklearn.base.RegressorMixin, LinearModelCV)\n",
      " |  LassoCV(*, eps=0.001, n_alphas=100, alphas=None, fit_intercept=True, normalize='deprecated', precompute='auto', max_iter=1000, tol=0.0001, copy_X=True, cv=None, verbose=False, n_jobs=None, positive=False, random_state=None, selection='cyclic')\n",
      " |  \n",
      " |  Lasso linear model with iterative fitting along a regularization path.\n",
      " |  \n",
      " |  See glossary entry for :term:`cross-validation estimator`.\n",
      " |  \n",
      " |  The best model is selected by cross-validation.\n",
      " |  \n",
      " |  The optimization objective for Lasso is::\n",
      " |  \n",
      " |      (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <lasso>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  eps : float, default=1e-3\n",
      " |      Length of the path. ``eps=1e-3`` means that\n",
      " |      ``alpha_min / alpha_max = 1e-3``.\n",
      " |  \n",
      " |  n_alphas : int, default=100\n",
      " |      Number of alphas along the regularization path.\n",
      " |  \n",
      " |  alphas : ndarray, default=None\n",
      " |      List of alphas where to compute the models.\n",
      " |      If ``None`` alphas are set automatically.\n",
      " |  \n",
      " |  fit_intercept : bool, default=True\n",
      " |      Whether to calculate the intercept for this model. If set\n",
      " |      to false, no intercept will be used in calculations\n",
      " |      (i.e. data is expected to be centered).\n",
      " |  \n",
      " |  normalize : bool, default=False\n",
      " |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      " |      If True, the regressors X will be normalized before regression by\n",
      " |      subtracting the mean and dividing by the l2-norm.\n",
      " |      If you wish to standardize, please use\n",
      " |      :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      " |      on an estimator with ``normalize=False``.\n",
      " |  \n",
      " |      .. deprecated:: 1.0\n",
      " |          ``normalize`` was deprecated in version 1.0 and will be removed in\n",
      " |          1.2.\n",
      " |  \n",
      " |  precompute : 'auto', bool or array-like of shape             (n_features, n_features), default='auto'\n",
      " |      Whether to use a precomputed Gram matrix to speed up\n",
      " |      calculations. If set to ``'auto'`` let us decide. The Gram\n",
      " |      matrix can also be passed as argument.\n",
      " |  \n",
      " |  max_iter : int, default=1000\n",
      " |      The maximum number of iterations.\n",
      " |  \n",
      " |  tol : float, default=1e-4\n",
      " |      The tolerance for the optimization: if the updates are\n",
      " |      smaller than ``tol``, the optimization code checks the\n",
      " |      dual gap for optimality and continues until it is smaller\n",
      " |      than ``tol``.\n",
      " |  \n",
      " |  copy_X : bool, default=True\n",
      " |      If ``True``, X will be copied; else, it may be overwritten.\n",
      " |  \n",
      " |  cv : int, cross-validation generator or iterable, default=None\n",
      " |      Determines the cross-validation splitting strategy.\n",
      " |      Possible inputs for cv are:\n",
      " |  \n",
      " |      - None, to use the default 5-fold cross-validation,\n",
      " |      - int, to specify the number of folds.\n",
      " |      - :term:`CV splitter`,\n",
      " |      - An iterable yielding (train, test) splits as arrays of indices.\n",
      " |  \n",
      " |      For int/None inputs, :class:`KFold` is used.\n",
      " |  \n",
      " |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      " |      cross-validation strategies that can be used here.\n",
      " |  \n",
      " |      .. versionchanged:: 0.22\n",
      " |          ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      " |  \n",
      " |  verbose : bool or int, default=False\n",
      " |      Amount of verbosity.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      Number of CPUs to use during the cross validation.\n",
      " |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      " |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      " |      for more details.\n",
      " |  \n",
      " |  positive : bool, default=False\n",
      " |      If positive, restrict regression coefficients to be positive.\n",
      " |  \n",
      " |  random_state : int, RandomState instance, default=None\n",
      " |      The seed of the pseudo random number generator that selects a random\n",
      " |      feature to update. Used when ``selection`` == 'random'.\n",
      " |      Pass an int for reproducible output across multiple function calls.\n",
      " |      See :term:`Glossary <random_state>`.\n",
      " |  \n",
      " |  selection : {'cyclic', 'random'}, default='cyclic'\n",
      " |      If set to 'random', a random coefficient is updated every iteration\n",
      " |      rather than looping over features sequentially by default. This\n",
      " |      (setting to 'random') often leads to significantly faster convergence\n",
      " |      especially when tol is higher than 1e-4.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  alpha_ : float\n",
      " |      The amount of penalization chosen by cross validation.\n",
      " |  \n",
      " |  coef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n",
      " |      Parameter vector (w in the cost function formula).\n",
      " |  \n",
      " |  intercept_ : float or ndarray of shape (n_targets,)\n",
      " |      Independent term in decision function.\n",
      " |  \n",
      " |  mse_path_ : ndarray of shape (n_alphas, n_folds)\n",
      " |      Mean square error for the test set on each fold, varying alpha.\n",
      " |  \n",
      " |  alphas_ : ndarray of shape (n_alphas,)\n",
      " |      The grid of alphas used for fitting.\n",
      " |  \n",
      " |  dual_gap_ : float or ndarray of shape (n_targets,)\n",
      " |      The dual gap at the end of the optimization for the optimal alpha\n",
      " |      (``alpha_``).\n",
      " |  \n",
      " |  n_iter_ : int\n",
      " |      Number of iterations run by the coordinate descent solver to reach\n",
      " |      the specified tolerance for the optimal alpha.\n",
      " |  \n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      " |      has feature names that are all strings.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  lars_path : Compute Least Angle Regression or Lasso path using LARS\n",
      " |      algorithm.\n",
      " |  lasso_path : Compute Lasso path with coordinate descent.\n",
      " |  Lasso : The Lasso is a linear model that estimates sparse coefficients.\n",
      " |  LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.\n",
      " |  LassoCV : Lasso linear model with iterative fitting along a regularization\n",
      " |      path.\n",
      " |  LassoLarsCV : Cross-validated Lasso using the LARS algorithm.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  For an example, see\n",
      " |  :ref:`examples/linear_model/plot_lasso_model_selection.py\n",
      " |  <sphx_glr_auto_examples_linear_model_plot_lasso_model_selection.py>`.\n",
      " |  \n",
      " |  To avoid unnecessary memory duplication the X argument of the fit method\n",
      " |  should be directly passed as a Fortran-contiguous numpy array.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.linear_model import LassoCV\n",
      " |  >>> from sklearn.datasets import make_regression\n",
      " |  >>> X, y = make_regression(noise=4, random_state=0)\n",
      " |  >>> reg = LassoCV(cv=5, random_state=0).fit(X, y)\n",
      " |  >>> reg.score(X, y)\n",
      " |  0.9993...\n",
      " |  >>> reg.predict(X[:1,])\n",
      " |  array([-78.4951...])\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LassoCV\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      LinearModelCV\n",
      " |      sklearn.base.MultiOutputMixin\n",
      " |      sklearn.linear_model._base.LinearModel\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, eps=0.001, n_alphas=100, alphas=None, fit_intercept=True, normalize='deprecated', precompute='auto', max_iter=1000, tol=0.0001, copy_X=True, cv=None, verbose=False, n_jobs=None, positive=False, random_state=None, selection='cyclic')\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  path = lasso_path(X, y, *, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, **params)\n",
      " |      Compute Lasso path with coordinate descent.\n",
      " |      \n",
      " |      The Lasso optimization function varies for mono and multi-outputs.\n",
      " |      \n",
      " |      For mono-output tasks it is::\n",
      " |      \n",
      " |          (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      " |      \n",
      " |      For multi-output tasks it is::\n",
      " |      \n",
      " |          (1 / (2 * n_samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21\n",
      " |      \n",
      " |      Where::\n",
      " |      \n",
      " |          ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      " |      \n",
      " |      i.e. the sum of norm of each row.\n",
      " |      \n",
      " |      Read more in the :ref:`User Guide <lasso>`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Training data. Pass directly as Fortran-contiguous data to avoid\n",
      " |          unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      " |          can be sparse.\n",
      " |      \n",
      " |      y : {array-like, sparse matrix} of shape (n_samples,) or         (n_samples, n_targets)\n",
      " |          Target values.\n",
      " |      \n",
      " |      eps : float, default=1e-3\n",
      " |          Length of the path. ``eps=1e-3`` means that\n",
      " |          ``alpha_min / alpha_max = 1e-3``.\n",
      " |      \n",
      " |      n_alphas : int, default=100\n",
      " |          Number of alphas along the regularization path.\n",
      " |      \n",
      " |      alphas : ndarray, default=None\n",
      " |          List of alphas where to compute the models.\n",
      " |          If ``None`` alphas are set automatically.\n",
      " |      \n",
      " |      precompute : 'auto', bool or array-like of shape             (n_features, n_features), default='auto'\n",
      " |          Whether to use a precomputed Gram matrix to speed up\n",
      " |          calculations. If set to ``'auto'`` let us decide. The Gram\n",
      " |          matrix can also be passed as argument.\n",
      " |      \n",
      " |      Xy : array-like of shape (n_features,) or (n_features, n_targets),         default=None\n",
      " |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      " |          only when the Gram matrix is precomputed.\n",
      " |      \n",
      " |      copy_X : bool, default=True\n",
      " |          If ``True``, X will be copied; else, it may be overwritten.\n",
      " |      \n",
      " |      coef_init : ndarray of shape (n_features, ), default=None\n",
      " |          The initial values of the coefficients.\n",
      " |      \n",
      " |      verbose : bool or int, default=False\n",
      " |          Amount of verbosity.\n",
      " |      \n",
      " |      return_n_iter : bool, default=False\n",
      " |          Whether to return the number of iterations or not.\n",
      " |      \n",
      " |      positive : bool, default=False\n",
      " |          If set to True, forces coefficients to be positive.\n",
      " |          (Only allowed when ``y.ndim == 1``).\n",
      " |      \n",
      " |      **params : kwargs\n",
      " |          Keyword arguments passed to the coordinate descent solver.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      alphas : ndarray of shape (n_alphas,)\n",
      " |          The alphas along the path where models are computed.\n",
      " |      \n",
      " |      coefs : ndarray of shape (n_features, n_alphas) or             (n_targets, n_features, n_alphas)\n",
      " |          Coefficients along the path.\n",
      " |      \n",
      " |      dual_gaps : ndarray of shape (n_alphas,)\n",
      " |          The dual gaps at the end of the optimization for each alpha.\n",
      " |      \n",
      " |      n_iters : list of int\n",
      " |          The number of iterations taken by the coordinate descent optimizer to\n",
      " |          reach the specified tolerance for each alpha.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      lars_path : Compute Least Angle Regression or Lasso path using LARS\n",
      " |          algorithm.\n",
      " |      Lasso : The Lasso is a linear model that estimates sparse coefficients.\n",
      " |      LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.\n",
      " |      LassoCV : Lasso linear model with iterative fitting along a regularization\n",
      " |          path.\n",
      " |      LassoLarsCV : Cross-validated Lasso using the LARS algorithm.\n",
      " |      sklearn.decomposition.sparse_encode : Estimator that can be used to\n",
      " |          transform signals into sparse linear combination of atoms from a fixed.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      For an example, see\n",
      " |      :ref:`examples/linear_model/plot_lasso_coordinate_descent_path.py\n",
      " |      <sphx_glr_auto_examples_linear_model_plot_lasso_coordinate_descent_path.py>`.\n",
      " |      \n",
      " |      To avoid unnecessary memory duplication the X argument of the fit method\n",
      " |      should be directly passed as a Fortran-contiguous numpy array.\n",
      " |      \n",
      " |      Note that in certain cases, the Lars solver may be significantly\n",
      " |      faster to implement this functionality. In particular, linear\n",
      " |      interpolation can be used to retrieve model coefficients between the\n",
      " |      values output by lars_path\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      \n",
      " |      Comparing lasso_path and lars_path with interpolation:\n",
      " |      \n",
      " |      >>> import numpy as np\n",
      " |      >>> from sklearn.linear_model import lasso_path\n",
      " |      >>> X = np.array([[1, 2, 3.1], [2.3, 5.4, 4.3]]).T\n",
      " |      >>> y = np.array([1, 2, 3.1])\n",
      " |      >>> # Use lasso_path to compute a coefficient path\n",
      " |      >>> _, coef_path, _ = lasso_path(X, y, alphas=[5., 1., .5])\n",
      " |      >>> print(coef_path)\n",
      " |      [[0.         0.         0.46874778]\n",
      " |       [0.2159048  0.4425765  0.23689075]]\n",
      " |      \n",
      " |      >>> # Now use lars_path and 1D linear interpolation to compute the\n",
      " |      >>> # same path\n",
      " |      >>> from sklearn.linear_model import lars_path\n",
      " |      >>> alphas, active, coef_path_lars = lars_path(X, y, method='lasso')\n",
      " |      >>> from scipy import interpolate\n",
      " |      >>> coef_path_continuous = interpolate.interp1d(alphas[::-1],\n",
      " |      ...                                             coef_path_lars[:, ::-1])\n",
      " |      >>> print(coef_path_continuous([5., 1., .5]))\n",
      " |      [[0.         0.         0.46915237]\n",
      " |       [0.2159048  0.4425765  0.23668876]]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the coefficient of determination of the prediction.\n",
      " |      \n",
      " |      The coefficient of determination :math:`R^2` is defined as\n",
      " |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      " |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      " |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      " |      The best possible score is 1.0 and it can be negative (because the\n",
      " |      model can be arbitrarily worse). A constant model that always predicts\n",
      " |      the expected value of `y`, disregarding the input features, would get\n",
      " |      a :math:`R^2` score of 0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples. For some estimators this may be a precomputed\n",
      " |          kernel matrix or a list of generic objects instead with shape\n",
      " |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      " |          is the number of samples used in the fitting for the estimator.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True values for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      " |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      " |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      " |      This influences the ``score`` method of all the multioutput\n",
      " |      regressors (except for\n",
      " |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from LinearModelCV:\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit linear model with coordinate descent.\n",
      " |      \n",
      " |      Fit is on grid of alphas and best alpha estimated by cross-validation.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Training data. Pass directly as Fortran-contiguous data\n",
      " |          to avoid unnecessary memory duplication. If y is mono-output,\n",
      " |          X can be sparse.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_targets)\n",
      " |          Target values.\n",
      " |      \n",
      " |      sample_weight : float or array-like of shape (n_samples,),                 default=None\n",
      " |          Sample weights used for fitting and evaluation of the weighted\n",
      " |          mean squared error of each cv-fold. Note that the cross validated\n",
      " |          MSE that is finally used to find the best model is the unweighted\n",
      " |          mean over the (weighted) MSEs of each test fold.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Returns an instance of fitted model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict using the linear model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      " |          Samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array, shape (n_samples,)\n",
      " |          Returns predicted values.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(LassoCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2634711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lassocv_model = LassoCV(eps= 0.1, n_alphas = 100, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3669deb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LassoCV(cv=5, eps=0.1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lassocv_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8f8f2a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = lassocv_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "723dece0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation metrics\n",
    "MAE = mean_absolute_error(y_test, test_predictions)\n",
    "MSE = mean_squared_error(y_test, test_predictions)\n",
    "RMSE = np.sqrt(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9ea297b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6541723161252858"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "deef4099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1308001022762542"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f0cc9e12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1308001022762542"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cec80420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69128071408207"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training set performance\n",
    "train_prediction = lassocv_model.predict(X_train)\n",
    "MAE = mean_absolute_error(y_train, train_prediction)\n",
    "MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b66407cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.002651  , 0.        , 0.        , 0.        , 3.79745279,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lassocv_model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7365ce9",
   "metadata": {},
   "source": [
    "## Elastic Net\n",
    "Elastic Net combines the penalties of ridge regression and lasso in an attempt to get the best of both worlds!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "39811b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNetCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b2585d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ElasticNetCV in module sklearn.linear_model._coordinate_descent:\n",
      "\n",
      "class ElasticNetCV(sklearn.base.RegressorMixin, LinearModelCV)\n",
      " |  ElasticNetCV(*, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, fit_intercept=True, normalize='deprecated', precompute='auto', max_iter=1000, tol=0.0001, cv=None, copy_X=True, verbose=0, n_jobs=None, positive=False, random_state=None, selection='cyclic')\n",
      " |  \n",
      " |  Elastic Net model with iterative fitting along a regularization path.\n",
      " |  \n",
      " |  See glossary entry for :term:`cross-validation estimator`.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <elastic_net>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  l1_ratio : float or list of float, default=0.5\n",
      " |      Float between 0 and 1 passed to ElasticNet (scaling between\n",
      " |      l1 and l2 penalties). For ``l1_ratio = 0``\n",
      " |      the penalty is an L2 penalty. For ``l1_ratio = 1`` it is an L1 penalty.\n",
      " |      For ``0 < l1_ratio < 1``, the penalty is a combination of L1 and L2\n",
      " |      This parameter can be a list, in which case the different\n",
      " |      values are tested by cross-validation and the one giving the best\n",
      " |      prediction score is used. Note that a good choice of list of\n",
      " |      values for l1_ratio is often to put more values close to 1\n",
      " |      (i.e. Lasso) and less close to 0 (i.e. Ridge), as in ``[.1, .5, .7,\n",
      " |      .9, .95, .99, 1]``.\n",
      " |  \n",
      " |  eps : float, default=1e-3\n",
      " |      Length of the path. ``eps=1e-3`` means that\n",
      " |      ``alpha_min / alpha_max = 1e-3``.\n",
      " |  \n",
      " |  n_alphas : int, default=100\n",
      " |      Number of alphas along the regularization path, used for each l1_ratio.\n",
      " |  \n",
      " |  alphas : ndarray, default=None\n",
      " |      List of alphas where to compute the models.\n",
      " |      If None alphas are set automatically.\n",
      " |  \n",
      " |  fit_intercept : bool, default=True\n",
      " |      Whether to calculate the intercept for this model. If set\n",
      " |      to false, no intercept will be used in calculations\n",
      " |      (i.e. data is expected to be centered).\n",
      " |  \n",
      " |  normalize : bool, default=False\n",
      " |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      " |      If True, the regressors X will be normalized before regression by\n",
      " |      subtracting the mean and dividing by the l2-norm.\n",
      " |      If you wish to standardize, please use\n",
      " |      :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      " |      on an estimator with ``normalize=False``.\n",
      " |  \n",
      " |      .. deprecated:: 1.0\n",
      " |          ``normalize`` was deprecated in version 1.0 and will be removed in\n",
      " |          1.2.\n",
      " |  \n",
      " |  precompute : 'auto', bool or array-like of shape             (n_features, n_features), default='auto'\n",
      " |      Whether to use a precomputed Gram matrix to speed up\n",
      " |      calculations. If set to ``'auto'`` let us decide. The Gram\n",
      " |      matrix can also be passed as argument.\n",
      " |  \n",
      " |  max_iter : int, default=1000\n",
      " |      The maximum number of iterations.\n",
      " |  \n",
      " |  tol : float, default=1e-4\n",
      " |      The tolerance for the optimization: if the updates are\n",
      " |      smaller than ``tol``, the optimization code checks the\n",
      " |      dual gap for optimality and continues until it is smaller\n",
      " |      than ``tol``.\n",
      " |  \n",
      " |  cv : int, cross-validation generator or iterable, default=None\n",
      " |      Determines the cross-validation splitting strategy.\n",
      " |      Possible inputs for cv are:\n",
      " |  \n",
      " |      - None, to use the default 5-fold cross-validation,\n",
      " |      - int, to specify the number of folds.\n",
      " |      - :term:`CV splitter`,\n",
      " |      - An iterable yielding (train, test) splits as arrays of indices.\n",
      " |  \n",
      " |      For int/None inputs, :class:`KFold` is used.\n",
      " |  \n",
      " |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      " |      cross-validation strategies that can be used here.\n",
      " |  \n",
      " |      .. versionchanged:: 0.22\n",
      " |          ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      " |  \n",
      " |  copy_X : bool, default=True\n",
      " |      If ``True``, X will be copied; else, it may be overwritten.\n",
      " |  \n",
      " |  verbose : bool or int, default=0\n",
      " |      Amount of verbosity.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      Number of CPUs to use during the cross validation.\n",
      " |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      " |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      " |      for more details.\n",
      " |  \n",
      " |  positive : bool, default=False\n",
      " |      When set to ``True``, forces the coefficients to be positive.\n",
      " |  \n",
      " |  random_state : int, RandomState instance, default=None\n",
      " |      The seed of the pseudo random number generator that selects a random\n",
      " |      feature to update. Used when ``selection`` == 'random'.\n",
      " |      Pass an int for reproducible output across multiple function calls.\n",
      " |      See :term:`Glossary <random_state>`.\n",
      " |  \n",
      " |  selection : {'cyclic', 'random'}, default='cyclic'\n",
      " |      If set to 'random', a random coefficient is updated every iteration\n",
      " |      rather than looping over features sequentially by default. This\n",
      " |      (setting to 'random') often leads to significantly faster convergence\n",
      " |      especially when tol is higher than 1e-4.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  alpha_ : float\n",
      " |      The amount of penalization chosen by cross validation.\n",
      " |  \n",
      " |  l1_ratio_ : float\n",
      " |      The compromise between l1 and l2 penalization chosen by\n",
      " |      cross validation.\n",
      " |  \n",
      " |  coef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n",
      " |      Parameter vector (w in the cost function formula).\n",
      " |  \n",
      " |  intercept_ : float or ndarray of shape (n_targets, n_features)\n",
      " |      Independent term in the decision function.\n",
      " |  \n",
      " |  mse_path_ : ndarray of shape (n_l1_ratio, n_alpha, n_folds)\n",
      " |      Mean square error for the test set on each fold, varying l1_ratio and\n",
      " |      alpha.\n",
      " |  \n",
      " |  alphas_ : ndarray of shape (n_alphas,) or (n_l1_ratio, n_alphas)\n",
      " |      The grid of alphas used for fitting, for each l1_ratio.\n",
      " |  \n",
      " |  dual_gap_ : float\n",
      " |      The dual gaps at the end of the optimization for the optimal alpha.\n",
      " |  \n",
      " |  n_iter_ : int\n",
      " |      Number of iterations run by the coordinate descent solver to reach\n",
      " |      the specified tolerance for the optimal alpha.\n",
      " |  \n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      " |      has feature names that are all strings.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  enet_path : Compute elastic net path with coordinate descent.\n",
      " |  ElasticNet : Linear regression with combined L1 and L2 priors as regularizer.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  For an example, see\n",
      " |  :ref:`examples/linear_model/plot_lasso_model_selection.py\n",
      " |  <sphx_glr_auto_examples_linear_model_plot_lasso_model_selection.py>`.\n",
      " |  \n",
      " |  To avoid unnecessary memory duplication the X argument of the fit method\n",
      " |  should be directly passed as a Fortran-contiguous numpy array.\n",
      " |  \n",
      " |  The parameter l1_ratio corresponds to alpha in the glmnet R package\n",
      " |  while alpha corresponds to the lambda parameter in glmnet.\n",
      " |  More specifically, the optimization objective is::\n",
      " |  \n",
      " |      1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      " |      + alpha * l1_ratio * ||w||_1\n",
      " |      + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      " |  \n",
      " |  If you are interested in controlling the L1 and L2 penalty\n",
      " |  separately, keep in mind that this is equivalent to::\n",
      " |  \n",
      " |      a * L1 + b * L2\n",
      " |  \n",
      " |  for::\n",
      " |  \n",
      " |      alpha = a + b and l1_ratio = a / (a + b).\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.linear_model import ElasticNetCV\n",
      " |  >>> from sklearn.datasets import make_regression\n",
      " |  \n",
      " |  >>> X, y = make_regression(n_features=2, random_state=0)\n",
      " |  >>> regr = ElasticNetCV(cv=5, random_state=0)\n",
      " |  >>> regr.fit(X, y)\n",
      " |  ElasticNetCV(cv=5, random_state=0)\n",
      " |  >>> print(regr.alpha_)\n",
      " |  0.199...\n",
      " |  >>> print(regr.intercept_)\n",
      " |  0.398...\n",
      " |  >>> print(regr.predict([[0, 0]]))\n",
      " |  [0.398...]\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      ElasticNetCV\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      LinearModelCV\n",
      " |      sklearn.base.MultiOutputMixin\n",
      " |      sklearn.linear_model._base.LinearModel\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, fit_intercept=True, normalize='deprecated', precompute='auto', max_iter=1000, tol=0.0001, cv=None, copy_X=True, verbose=0, n_jobs=None, positive=False, random_state=None, selection='cyclic')\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  path = enet_path(X, y, *, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, check_input=True, **params)\n",
      " |      Compute elastic net path with coordinate descent.\n",
      " |      \n",
      " |      The elastic net optimization function varies for mono and multi-outputs.\n",
      " |      \n",
      " |      For mono-output tasks it is::\n",
      " |      \n",
      " |          1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      " |          + alpha * l1_ratio * ||w||_1\n",
      " |          + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      " |      \n",
      " |      For multi-output tasks it is::\n",
      " |      \n",
      " |          (1 / (2 * n_samples)) * ||Y - XW||_Fro^2\n",
      " |          + alpha * l1_ratio * ||W||_21\n",
      " |          + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n",
      " |      \n",
      " |      Where::\n",
      " |      \n",
      " |          ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      " |      \n",
      " |      i.e. the sum of norm of each row.\n",
      " |      \n",
      " |      Read more in the :ref:`User Guide <elastic_net>`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Training data. Pass directly as Fortran-contiguous data to avoid\n",
      " |          unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      " |          can be sparse.\n",
      " |      \n",
      " |      y : {array-like, sparse matrix} of shape (n_samples,) or         (n_samples, n_targets)\n",
      " |          Target values.\n",
      " |      \n",
      " |      l1_ratio : float, default=0.5\n",
      " |          Number between 0 and 1 passed to elastic net (scaling between\n",
      " |          l1 and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso.\n",
      " |      \n",
      " |      eps : float, default=1e-3\n",
      " |          Length of the path. ``eps=1e-3`` means that\n",
      " |          ``alpha_min / alpha_max = 1e-3``.\n",
      " |      \n",
      " |      n_alphas : int, default=100\n",
      " |          Number of alphas along the regularization path.\n",
      " |      \n",
      " |      alphas : ndarray, default=None\n",
      " |          List of alphas where to compute the models.\n",
      " |          If None alphas are set automatically.\n",
      " |      \n",
      " |      precompute : 'auto', bool or array-like of shape             (n_features, n_features), default='auto'\n",
      " |          Whether to use a precomputed Gram matrix to speed up\n",
      " |          calculations. If set to ``'auto'`` let us decide. The Gram\n",
      " |          matrix can also be passed as argument.\n",
      " |      \n",
      " |      Xy : array-like of shape (n_features,) or (n_features, n_targets),         default=None\n",
      " |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      " |          only when the Gram matrix is precomputed.\n",
      " |      \n",
      " |      copy_X : bool, default=True\n",
      " |          If ``True``, X will be copied; else, it may be overwritten.\n",
      " |      \n",
      " |      coef_init : ndarray of shape (n_features, ), default=None\n",
      " |          The initial values of the coefficients.\n",
      " |      \n",
      " |      verbose : bool or int, default=False\n",
      " |          Amount of verbosity.\n",
      " |      \n",
      " |      return_n_iter : bool, default=False\n",
      " |          Whether to return the number of iterations or not.\n",
      " |      \n",
      " |      positive : bool, default=False\n",
      " |          If set to True, forces coefficients to be positive.\n",
      " |          (Only allowed when ``y.ndim == 1``).\n",
      " |      \n",
      " |      check_input : bool, default=True\n",
      " |          If set to False, the input validation checks are skipped (including the\n",
      " |          Gram matrix when provided). It is assumed that they are handled\n",
      " |          by the caller.\n",
      " |      \n",
      " |      **params : kwargs\n",
      " |          Keyword arguments passed to the coordinate descent solver.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      alphas : ndarray of shape (n_alphas,)\n",
      " |          The alphas along the path where models are computed.\n",
      " |      \n",
      " |      coefs : ndarray of shape (n_features, n_alphas) or             (n_targets, n_features, n_alphas)\n",
      " |          Coefficients along the path.\n",
      " |      \n",
      " |      dual_gaps : ndarray of shape (n_alphas,)\n",
      " |          The dual gaps at the end of the optimization for each alpha.\n",
      " |      \n",
      " |      n_iters : list of int\n",
      " |          The number of iterations taken by the coordinate descent optimizer to\n",
      " |          reach the specified tolerance for each alpha.\n",
      " |          (Is returned when ``return_n_iter`` is set to True).\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      MultiTaskElasticNet : Multi-task ElasticNet model trained with L1/L2 mixed-norm     as regularizer.\n",
      " |      MultiTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in cross-validation.\n",
      " |      ElasticNet : Linear regression with combined L1 and L2 priors as regularizer.\n",
      " |      ElasticNetCV : Elastic Net model with iterative fitting along a regularization path.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      For an example, see\n",
      " |      :ref:`examples/linear_model/plot_lasso_coordinate_descent_path.py\n",
      " |      <sphx_glr_auto_examples_linear_model_plot_lasso_coordinate_descent_path.py>`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the coefficient of determination of the prediction.\n",
      " |      \n",
      " |      The coefficient of determination :math:`R^2` is defined as\n",
      " |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      " |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      " |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      " |      The best possible score is 1.0 and it can be negative (because the\n",
      " |      model can be arbitrarily worse). A constant model that always predicts\n",
      " |      the expected value of `y`, disregarding the input features, would get\n",
      " |      a :math:`R^2` score of 0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples. For some estimators this may be a precomputed\n",
      " |          kernel matrix or a list of generic objects instead with shape\n",
      " |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      " |          is the number of samples used in the fitting for the estimator.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True values for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      " |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      " |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      " |      This influences the ``score`` method of all the multioutput\n",
      " |      regressors (except for\n",
      " |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from LinearModelCV:\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit linear model with coordinate descent.\n",
      " |      \n",
      " |      Fit is on grid of alphas and best alpha estimated by cross-validation.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Training data. Pass directly as Fortran-contiguous data\n",
      " |          to avoid unnecessary memory duplication. If y is mono-output,\n",
      " |          X can be sparse.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_targets)\n",
      " |          Target values.\n",
      " |      \n",
      " |      sample_weight : float or array-like of shape (n_samples,),                 default=None\n",
      " |          Sample weights used for fitting and evaluation of the weighted\n",
      " |          mean squared error of each cv-fold. Note that the cross validated\n",
      " |          MSE that is finally used to find the best model is the unweighted\n",
      " |          mean over the (weighted) MSEs of each test fold.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Returns an instance of fitted model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict using the linear model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      " |          Samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array, shape (n_samples,)\n",
      " |          Returns predicted values.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ElasticNetCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e381fcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_model = ElasticNetCV(l1_ratio= [.1, .5, .7,.9, .95, .99, 1], tol = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "36ee42ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElasticNetCV(l1_ratio=[0.1, 0.5, 0.7, 0.9, 0.95, 0.99, 1], tol=0.01)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elastic_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "db60d975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elastic_model.l1_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "04ab75f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = elastic_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4c9e1f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation metrics\n",
    "MAE = mean_absolute_error(y_test, test_predictions)\n",
    "MSE = mean_squared_error(y_test, test_predictions)\n",
    "RMSE = np.sqrt(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dfcbb724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5663262117569451"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d2e36da0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5603340214638839"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d50a57fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7485546215633726"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a5662a6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4307582990472368"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training set peformance\n",
    "train_predictions = elastic_model.predict(X_train)\n",
    "MAE = mean_absolute_error(y_train, train_predictions)\n",
    "MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "226f150f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.78993643,  0.89232919,  0.28765395, -1.01843566,  2.15516144,\n",
       "       -0.3567547 , -0.271502  ,  0.09741081,  0.        , -1.05563151,\n",
       "        0.2362506 ,  0.07980911,  1.26170778,  0.01464706,  0.00462336,\n",
       "       -0.39986069,  0.        ,  0.        , -0.05343757])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elastic_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd372f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
